{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\n#from sklearn.model_selection import train_test_split\nfrom zipfile import ZipFile\nfrom skimage.transform import resize\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, GlobalMaxPooling2D\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"traindf = pd.read_csv('../input/galaxy-zoo-the-galaxy-challenge/training_solutions_rev1.zip')\n\n#df_train, df_test = train_test_split(traindf, test_size=.1)\n#df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\n\nwith zipfile.ZipFile(\"/kaggle/input/galaxy-zoo-the-galaxy-challenge/images_training_rev1.zip\",\"r\") as z:\n    z.extractall(\".\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def append_ext(fn):\n    return fn + \".jpg\"\n\ntraindf[\"id\"] = traindf['GalaxyID'].astype(str).apply(append_ext)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image(image):\n  #Image - numpy array of rank 3\n  #Crop image to half its size, preserving the center\n  #resize image to shape\n  im_size_x = image.shape[0]\n  im_size_y = image.shape[0]\n  cropx = im_size_x // 4\n  cropy = im_size_y // 4\n  image = image[cropx:im_size_x-cropx, cropy:im_size_y-cropy]\n\n  re_shape = [64,64]\n  image = resize(image, re_shape)\n  return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#########################################\n## Import data with ImageDataGenerator\ndatagen = ImageDataGenerator(\n    fill_mode='constant',\n    cval=0,\n    rescale=1. / 255,\n    #rotation_range=90,\n    #width_shift_range=0.1,\n    #height_shift_range=0.1,\n    #horizontal_flip=True,\n    #vertical_flip=True,\n    preprocessing_function=crop_image,\n    validation_split=0.2)\n\nimage_dir = './images_training_rev1/'\nclasses = ['Class1.1', 'Class1.2', 'Class1.3', 'Class2.1', 'Class2.2', 'Class3.1','Class3.2', \n           'Class4.1', 'Class4.2', 'Class5.1', 'Class5.2', 'Class5.3','Class5.4', 'Class6.1', \n           'Class6.2', 'Class7.1', 'Class7.2', 'Class7.3','Class8.1', 'Class8.2', 'Class8.3', \n           'Class8.4', 'Class8.5', 'Class8.6', 'Class8.7', 'Class9.1', 'Class9.2', 'Class9.3', \n           'Class10.1', 'Class10.2', 'Class10.3', 'Class11.1', 'Class11.2', 'Class11.3', \n           'Class11.4', 'Class11.5', 'Class11.6']\n\ntrain_generator = datagen.flow_from_dataframe(\n    dataframe=traindf,\n    directory=image_dir,\n    x_col=\"id\",\n    y_col=classes,\n    subset=\"training\",\n    batch_size=64,\n    seed=123,\n    shuffle=True,\n    class_mode=\"raw\",\n    target_size=(64, 64))\n\nvalid_generator = datagen.flow_from_dataframe(\n    dataframe=traindf,\n    directory=image_dir,\n    x_col=\"id\",\n    y_col=classes,\n    subset=\"validation\",\n    batch_size=64,\n    seed=123,\n    shuffle=True,\n    class_mode=\"raw\",\n    target_size=(64, 64))\n\nSTEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\nmodel = Sequential()\nmodel.add(Conv2D(512, (3, 3), input_shape=(64, 64, 3)))\nmodel.add(Conv2D(256, (3, 3)))\n#model.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(256, (3, 3)))\nmodel.add(Conv2D(128, (3, 3)))\n#model.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, (3, 3)))\nmodel.add(Conv2D(128, (3, 3)))\n#model.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(GlobalMaxPooling2D())\n\n\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(37))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adamax', metrics=[root_mean_squared_error])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = './'\n\nclass LossHistory(Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.val_losses = []\n \n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss', patience=4, verbose=1, mode='auto')\n\nhistory = LossHistory()\n\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpointer = ModelCheckpoint(\n    filepath=data_dir+'weights_full.hdf5', verbose=2, save_best_only=True)\n\nhist = model.fit(\n    train_generator,\n    steps_per_epoch=STEP_SIZE_TRAIN,\n    validation_data=valid_generator,\n    validation_steps=STEP_SIZE_VALID,\n    epochs=15,\n    callbacks=[history, checkpointer, early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"my_h5_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting training and validation loss\nplt.figure(figsize=(12, 8))\nplt.plot(hist.epoch, hist.history['loss'], label='Training Loss')\nplt.plot(\n    hist.epoch, hist.history['val_loss'], label='Validation', linestyle='--')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"RMSE\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\n\nwith zipfile.ZipFile(\"/kaggle/input/galaxy-zoo-the-galaxy-challenge/images_test_rev1.zip\",\"r\") as z:\n    z.extractall(\".\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf = pd.read_csv('../input/galaxy-zoo-the-galaxy-challenge/all_zeros_benchmark.zip')\ntestdf[\"id\"] = testdf['GalaxyID'].astype(str).apply(append_ext)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ycols = ['GalaxyID'] + classes\n\n## Import test data with ImageDataGenerator\ntestdatagen = ImageDataGenerator(\n    fill_mode='constant',\n    cval=0,\n    rescale=1. / 255,\n    #rotation_range=90,\n    #width_shift_range=0.1,\n    #height_shift_range=0.1,\n    #horizontal_flip=True,\n    #vertical_flip=True,\n    preprocessing_function=crop_image,\n    validation_split=0.0)\n\ntest_image_dir = './images_test_rev1/'\n\ntest_generator = testdatagen.flow_from_dataframe(\n    dataframe=testdf,\n    directory=test_image_dir,\n    x_col=\"id\",\n    y_col=ycols,\n    subset=\"training\",\n    batch_size=64,\n    seed=123,\n    shuffle=True,\n    class_mode=\"raw\",\n    target_size=(64, 64))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_batches = len(test_generator)\nnum_in_batch = test_generator[0][0].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom numpy import expand_dims\n\nval_predictions = []\nids = []\n\n##for i in tqdm(range(n_batches)):\nfor i in tqdm(range(50)):\n    for j in range(num_in_batch):\n        img = test_generator[i][0][j]\n        img = expand_dims(img, axis=0)\n        y_pred = model.predict(img)\n        \n        val_predictions.append(y_pred)\n        ids.append(test_generator[i][1][j][0])\n        \nval_predictions = np.array(val_predictions)\nY_pred = np.vstack(val_predictions)\nids = np.array(ids).reshape(len(ids),1)\n\n#print(Y_pred)\n#print(ids)\n\nsubmission_df = pd.DataFrame(np.hstack((ids, Y_pred)), columns=testdf.columns[0:38])\nsubmission_df = submission_df.sort_values(by=['GalaxyID'])\nsubmission_df['GalaxyID'] = submission_df['GalaxyID'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('test_submission_1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\nmodel = load_model(data_dir+\"my_h5_model.h5\", custom_objects={'root_mean_squared_error': root_mean_squared_error})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = test_generator[6][0][10]\nimg = expand_dims(img, axis=0)\ntest_pred = model.predict(img)\nprint(test_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}